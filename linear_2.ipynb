{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\program files\\python312\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in d:\\program files\\python312\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: filelock in d:\\program files\\python312\\lib\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\program files\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in d:\\program files\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in d:\\program files\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\program files\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\program files\\python312\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in d:\\program files\\python312\\lib\\site-packages (from torch) (71.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\program files\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\program files\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install numpy torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've implemented linear regression & gradient descent model using some basic tensor operations. However, since this is a common pattern in deep learning, PyTorch provides several built-in functions and classes to make it easy to create and train models with just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides several built-in functions and classes to make it easy to create and train models with just a few lines of code.\n",
    "Let's begin by importing the torch.nn package from PyTorch, which contains utility classes for building neural networks.\n",
    "We are using 15 training examples to illustrate how to work with large datasets in small batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70], \n",
    "                   [74, 66, 43], \n",
    "                   [91, 87, 65], \n",
    "                   [88, 134, 59], \n",
    "                   [101, 44, 37], \n",
    "                   [68, 96, 71], \n",
    "                   [73, 66, 44], \n",
    "                   [92, 87, 64], \n",
    "                   [87, 135, 57], \n",
    "                   [103, 43, 36], \n",
    "                   [68, 97, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119],\n",
    "                    [57, 69], \n",
    "                    [80, 102], \n",
    "                    [118, 132], \n",
    "                    [21, 38], \n",
    "                    [104, 118], \n",
    "                    [57, 69], \n",
    "                    [82, 100], \n",
    "                    [118, 134], \n",
    "                    [20, 38], \n",
    "                    [102, 120]], \n",
    "                   dtype='float32')\n",
    "\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a TensorDataset, which allows access to rows from inputs and targets as tuples, and provides standard APIs for working with many different types of datasets in PyTorch\n",
    "The TensorDataset allows us to access a small section of the training data using the array indexing notation ([0:3] in the above code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define datasets\n",
    "train_ds=TensorDataset(inputs,targets)\n",
    "train_ds[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create a DataLoader, which can split the data into batches of a predefined size while training. It also provides other utilities like shuffling and random sampling of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# define data loader\n",
    "batch_size=5\n",
    "train_dl=DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 87., 135.,  57.],\n",
      "        [ 88., 134.,  59.],\n",
      "        [ 73.,  66.,  44.],\n",
      "        [ 69.,  96.,  70.],\n",
      "        [ 68.,  96.,  71.]])\n",
      "tensor([[118., 134.],\n",
      "        [118., 132.],\n",
      "        [ 57.,  69.],\n",
      "        [103., 119.],\n",
      "        [104., 118.]])\n"
     ]
    }
   ],
   "source": [
    "for xb ,yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Suffle can be used to randomized the input and otimizes the output leading to faster reduction in loss.\n",
    " previously we can add weight and baises manually, but their we can use build in function like nn.linear class from pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2214,  0.5714,  0.0151],\n",
      "        [ 0.1531, -0.4687, -0.3840]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1195, -0.5608], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model=nn.Linear(3,2)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 22.8947, -37.2946],\n",
       "        [ 31.2275, -52.4443],\n",
       "        [ 58.3080, -72.3113],\n",
       "        [  2.6696, -19.3014],\n",
       "        [ 40.7601, -61.8669],\n",
       "        [ 22.1019, -36.6728],\n",
       "        [ 30.6712, -52.3597],\n",
       "        [ 58.1018, -72.5422],\n",
       "        [  3.4624, -19.9232],\n",
       "        [ 40.9966, -62.4041],\n",
       "        [ 22.3384, -37.2099],\n",
       "        [ 30.4347, -51.8225],\n",
       "        [ 58.8643, -72.3959],\n",
       "        [  2.4331, -18.7642],\n",
       "        [ 41.5529, -62.4887]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate prediction as we do \n",
    "predict=model(inputs)\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use buildin function for the loss function\n",
    "# Import nn.functional\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12453.9648, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# The nn.functional package contains many useful loss functions and several other utilities\n",
    "# Define loss function\n",
    "loss_fn = F.mse_loss\n",
    "\n",
    "# lets compute the loss\n",
    "loss=loss_fn(model(inputs),targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for optimization\n",
    "# Define optimizer\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to train the model\n",
    "def train(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    for epoch in range(num_epochs):\n",
    "        # train with batches of data\n",
    "        for xb, yb in train_dl:\n",
    "\n",
    "            # make prediction\n",
    "            pred=model(xb)\n",
    "\n",
    "            # make loss function\n",
    "            loss=loss_fn(pred,yb)\n",
    "\n",
    "            #compute gradient\n",
    "            loss.backward()\n",
    "\n",
    "            # update gradient\n",
    "            opt.step()\n",
    "\n",
    "            #reset gradient\n",
    "\n",
    "            opt.zero_grad()\n",
    "\n",
    "            #print the progess\n",
    "\n",
    "        if (epoch+1)%10==0:\n",
    "            print('Epoch [{}/{}], Loss: {:.2f}'.format(epoch+1, num_epochs, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 366.53\n",
      "Epoch [20/200], Loss: 511.62\n",
      "Epoch [30/200], Loss: 273.55\n",
      "Epoch [40/200], Loss: 153.73\n",
      "Epoch [50/200], Loss: 99.52\n",
      "Epoch [60/200], Loss: 85.40\n",
      "Epoch [70/200], Loss: 71.19\n",
      "Epoch [80/200], Loss: 55.19\n",
      "Epoch [90/200], Loss: 44.23\n",
      "Epoch [100/200], Loss: 32.77\n",
      "Epoch [110/200], Loss: 26.92\n",
      "Epoch [120/200], Loss: 15.28\n",
      "Epoch [130/200], Loss: 17.15\n",
      "Epoch [140/200], Loss: 25.04\n",
      "Epoch [150/200], Loss: 24.89\n",
      "Epoch [160/200], Loss: 9.82\n",
      "Epoch [170/200], Loss: 35.50\n",
      "Epoch [180/200], Loss: 25.44\n",
      "Epoch [190/200], Loss: 20.06\n",
      "Epoch [200/200], Loss: 12.35\n"
     ]
    }
   ],
   "source": [
    "train(200,model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 57.3560,  70.9991],\n",
      "        [ 80.0707,  97.9642],\n",
      "        [122.5432, 138.6527],\n",
      "        [ 22.4021,  40.7326],\n",
      "        [ 97.5081, 112.1037],\n",
      "        [ 56.0737,  69.8966],\n",
      "        [ 79.5403,  97.5238],\n",
      "        [122.6283, 138.9783],\n",
      "        [ 23.6844,  41.8352],\n",
      "        [ 98.2601, 112.7659],\n",
      "        [ 56.8256,  70.5587],\n",
      "        [ 78.7884,  96.8616],\n",
      "        [123.0736, 139.0931],\n",
      "        [ 21.6502,  40.0705],\n",
      "        [ 98.7905, 113.2063]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "predict=model(inputs)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.],\n",
      "        [ 57.,  69.],\n",
      "        [ 80., 102.],\n",
      "        [118., 132.],\n",
      "        [ 21.,  38.],\n",
      "        [104., 118.],\n",
      "        [ 57.,  69.],\n",
      "        [ 82., 100.],\n",
      "        [118., 134.],\n",
      "        [ 20.,  38.],\n",
      "        [102., 120.]])\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
